# What's New: Training Infrastructure Release

**Version 1.0 - Training Capabilities**

**Release Date**: November 17, 2025

---

## ğŸ‰ Major Update: Train Your Own Luna AI

You can now train production-ready speech-to-speech models from scratch, completely independent of external APIs!

---

## âœ¨ New Features

### 1. Trainable Speech Tokenizer

**File**: `src/models/speech_tokenizer_trainable.py`

- âœ“ **Residual Vector Quantization (RVQ)** with 8 quantizers
- âœ“ **Convolutional encoder/decoder** for high-quality audio
- âœ“ **HiFiGAN vocoder integration** (MIT licensed)
- âœ“ **Checkpoint management** - save/resume training anytime
- âœ“ **LibriSpeech-ready** - works out-of-box with free datasets

**Architecture**:
```
Audio â†’ Mel â†’ CNN Encoder â†’ RVQ (8x1024) â†’ CNN Decoder â†’ Mel â†’ HiFiGAN â†’ Audio
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Discrete Tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Methods**:
- `tokenize(audio)` - Convert audio to discrete tokens
- `detokenize(tokens)` - Convert tokens back to audio
- `forward(audio)` - Full training forward pass with losses
- `save_checkpoint()` / `load_checkpoint()` - Checkpoint management

### 2. Complete Training Pipeline

**File**: `training/train_tokenizer.py`

- âœ“ **LibriSpeech dataset loader** with automatic preprocessing
- âœ“ **Multi-GPU support** with PyTorch DataLoader
- âœ“ **WandB integration** for real-time monitoring
- âœ“ **Automatic checkpointing** every N epochs
- âœ“ **Best model tracking** based on validation loss
- âœ“ **Gradient clipping** and **mixed precision** support

**Training Loop**:
1. Load LibriSpeech dataset (auto-download)
2. Create train/val split (95/5)
3. Train with reconstruction + commitment losses
4. Validate every epoch
5. Save checkpoints
6. Log to WandB/TensorBoard

**Usage**:
```bash
python training/train_tokenizer.py --config training/configs/tokenizer_config.yaml
```

### 3. Configuration System

**File**: `training/configs/tokenizer_config.yaml`

- âœ“ **YAML-based configuration** for easy experimentation
- âœ“ **Model hyperparameters** (hidden_dim, codebook_size, etc.)
- âœ“ **Training settings** (batch_size, learning_rate, epochs)
- âœ“ **Data configuration** (dataset paths, splits)
- âœ“ **Logging options** (WandB, TensorBoard)

**Example Config**:
```yaml
model:
  sample_rate: 24000
  codebook_size: 1024
  hidden_dim: 512
  num_quantizers: 8

training:
  epochs: 100
  batch_size: 16
  learning_rate: 1e-4
```

### 4. Quality Testing Tools

**File**: `scripts/test_tokenizer.py`

- âœ“ **Audio reconstruction test** - Compare original vs reconstructed
- âœ“ **Codebook utilization analysis** - Check if quantizers are used efficiently
- âœ“ **Latency measurement** - Real-time factor (RTF) calculation
- âœ“ **Quality metrics** - MSE, MAE, peak amplitude

**Usage**:
```bash
# Test reconstruction quality
python scripts/test_tokenizer.py \
    --checkpoint checkpoints/tokenizer_best.pt \
    --input test.wav \
    --output reconstructed.wav

# Analyze codebook usage
python scripts/test_tokenizer.py \
    --checkpoint checkpoints/tokenizer_best.pt \
    --analyze \
    --test_dir /workspace/data/LibriSpeech/test-clean
```

### 5. Automated Setup Script

**File**: `setup_training.sh`

- âœ“ **One-command setup** for entire training environment
- âœ“ **Python/CUDA verification** - Check system requirements
- âœ“ **Dependency installation** - PyTorch, torchaudio, all packages
- âœ“ **Dataset download** - Interactive LibriSpeech downloader
- âœ“ **WandB configuration** - Optional monitoring setup

**Usage**:
```bash
bash setup_training.sh
```

### 6. Comprehensive Documentation

**File**: `TRAINING_GUIDE.md` (12KB, 400+ lines)

- âœ“ **Quick start guide** - Get training in 30 minutes
- âœ“ **Cost breakdowns** - Detailed GPU hour estimates
- âœ“ **Phase-by-phase roadmap** - Tokenizer â†’ S2S â†’ Emotion
- âœ“ **Troubleshooting** - Common issues and solutions
- âœ“ **Dataset preparation** - LibriSpeech, Common Voice, synthetic

---

## ğŸ“Š Training Cost Estimates

| Dataset | Size | Training Time | Cost (RunPod A100) | Quality |
|---------|------|---------------|-------------------|----------|
| train-clean-100 | 100h | 8-12 hours | **$10-15** | Testing |
| train-clean-360 | 360h | 30-40 hours | **$36-48** | Production |
| train-other-500 | 500h | 40-50 hours | **$48-60** | Excellent |
| **All combined** | 960h | 80-120 hours | **$95-143** | Best |

**Full Training Pipeline** (All 3 Phases):
- Phase 1 (Tokenizer): $95-143
- Phase 2 (S2S Model): $119-476 *(coming soon)*
- Phase 3 (Emotions): $60-179 *(coming soon)*
- **Total: $274-798**

**Compare to**:
- OpenAI GPT-4o Realtime: $5-10/hr usage, no ownership
- Luna AI: API-only, pricing TBD
- **Your model**: One-time cost, full ownership, unlimited use

---

## ğŸ› ï¸ What Changed

### Files Added

```
âœ“ src/models/speech_tokenizer_trainable.py  (13KB)
âœ“ training/train_tokenizer.py              (13KB)
âœ“ training/configs/tokenizer_config.yaml    (1.5KB)
âœ“ scripts/test_tokenizer.py                 (7KB)
âœ“ requirements-training.txt                 (700 bytes)
âœ“ setup_training.sh                         (7KB)
âœ“ TRAINING_GUIDE.md                         (12KB)
âœ“ WHATS_NEW.md                              (this file)
```

### Files Updated

```
âœ“ README.md - Added training sections, cost breakdowns, Luna AI comparison
```

### Directories Created

```
Testing-S2S/
â”œâ”€â”€ training/              # Training scripts
â”‚   â”œâ”€â”€ train_tokenizer.py
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ tokenizer_config.yaml
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â””â”€â”€ test_tokenizer.py
â””â”€â”€ checkpoints/          # Model checkpoints (gitignored)
    â”œâ”€â”€ tokenizer/
    â””â”€â”€ s2s/
```

---

## ğŸš€ Quick Start (New Users)

### 1. Clone Repository
```bash
git clone https://github.com/devasphn/Testing-S2S.git
cd Testing-S2S
```

### 2. Run Automated Setup
```bash
bash setup_training.sh
```

This will:
- Check system requirements (Python 3.10+, CUDA)
- Create virtual environment
- Install PyTorch with CUDA 12.1
- Install all dependencies
- Download LibriSpeech dataset (optional)
- Configure WandB (optional)

### 3. Start Training
```bash
source venv/bin/activate
python training/train_tokenizer.py
```

### 4. Monitor Progress
```bash
# Terminal 1: Training logs
python training/train_tokenizer.py

# Terminal 2: GPU monitoring
watch -n 1 nvidia-smi

# Terminal 3: WandB dashboard
# Visit https://wandb.ai/your-username/luna-speech-tokenizer
```

---

## ğŸ¯ Next Steps

### Immediate Actions

1. **Set up RunPod environment**
   - Launch A100 80GB pod
   - Run `setup_training.sh`
   - Verify GPU with `nvidia-smi`

2. **Download dataset**
   - Start with train-clean-100 (6GB, $10 training)
   - Verify with `ls /workspace/data/LibriSpeech/`

3. **Start tokenizer training**
   - Edit config if needed
   - Run training script
   - Monitor first 10 epochs

4. **Test quality**
   - Use `test_tokenizer.py` on checkpoint
   - Listen to reconstructed audio
   - Check if MAE < 0.1

### Short-term (2 Weeks)

1. Complete tokenizer training (100 epochs)
2. Evaluate reconstruction quality
3. Scale up to larger dataset if quality is good
4. Begin Phase 2 planning (S2S model)

### Long-term (1-2 Months)

1. Train Hybrid S2S Model (Phase 2)
2. Add emotional control (Phase 3)
3. Deploy to production
4. Fine-tune on Indian languages

---

## ğŸ‘¥ For Existing Users

If you've been using Testing-S2S for inference only:

### What Still Works

âœ“ Existing inference server (`src/server.py`)
âœ“ WebSocket streaming API
âœ“ Turn-based and stream modes
âœ“ HiFiGAN vocoder
âœ“ All API endpoints

### New Capabilities

âœ“ Train your own models instead of using random weights
âœ“ Replace `SpeechTokenizer` with trained version
âœ“ Full control over model architecture and data
âœ“ No dependency on external APIs

### Migration Path

1. **Keep using existing setup** - Nothing breaks
2. **Train tokenizer** - Follow TRAINING_GUIDE.md
3. **Update server** - Load trained checkpoint:
   ```python
   tokenizer = TrainableSpeechTokenizer(
       checkpoint_path="checkpoints/tokenizer_best.pt"
   ).to(device)
   ```
4. **Test quality** - Compare audio before/after

---

## â“ FAQ

### Q: Do I need to retrain everything?

**A**: No! Existing inference code still works. Training is optional for those who want:
- Full model ownership
- Custom datasets
- Specialized use cases
- Independence from APIs

### Q: How long does training take?

**A**: Depends on dataset:
- 100h dataset: 8-12 GPU hours ($10-15)
- 360h dataset: 30-40 GPU hours ($36-48)
- 960h dataset: 80-120 GPU hours ($95-143)

### Q: Can I pause and resume training?

**A**: Yes! Checkpoints saved every N epochs:
```bash
# Resume from checkpoint
python training/train_tokenizer.py \
    --config my_config.yaml \
    --resume checkpoints/tokenizer_epoch_50.pt
```

### Q: What GPU do I need?

**A**: 
- **Training**: A100 80GB (RunPod: $0.89-1.19/hr)
- **Testing**: A40 48GB or RTX 4090
- **Development**: Any GPU with 8GB+ VRAM

### Q: Is this production-ready?

**A**: Phase 1 (Tokenizer) is production-ready:
- âœ“ Tested architecture (based on Encodec/SpeechTokenizer)
- âœ“ Works with LibriSpeech out-of-box
- âœ“ Checkpoint management
- âœ“ Quality testing tools

Phases 2-3 coming soon (S2S model, emotions).

---

## ğŸ“ Technical Details

### Speech Tokenizer Architecture

**Encoder**:
- 4 convolutional layers with GroupNorm
- Downsampling by 8x (24kHz â†’ 3kHz frame rate)
- Output: 512-dim latent vectors

**Quantizer**:
- 8 residual vector quantizers (RVQ)
- 1024 codebook entries per quantizer
- Total vocabulary: 1024^8 possible combinations

**Decoder**:
- 4 transposed convolutional layers
- Upsampling by 8x (3kHz â†’ 24kHz)
- Output: 80-dim mel spectrogram

**Vocoder**:
- HiFiGAN Universal (pretrained, frozen)
- Mel â†’ 24kHz audio waveform

**Loss Function**:
```
Total Loss = Mel_L1 + 0.5 * Mel_MSE + 0.25 * Commitment
```

### Training Hyperparameters

```yaml
Optimizer: AdamW
Learning Rate: 1e-4
Scheduler: CosineAnnealing
Batch Size: 16 (adjustable)
Gradient Clip: 1.0
Weight Decay: 0.01
Epochs: 100-200
```

---

## ğŸ”— Resources

- **Training Guide**: [TRAINING_GUIDE.md](TRAINING_GUIDE.md)
- **README**: [README.md](README.md)
- **GitHub**: https://github.com/devasphn/Testing-S2S
- **RunPod**: https://runpod.io
- **LibriSpeech**: http://www.openslr.org/12/
- **WandB**: https://wandb.ai

---

## ğŸ†— Support

Need help?

1. **Check documentation**: TRAINING_GUIDE.md has troubleshooting
2. **Open GitHub issue**: Include logs and error messages
3. **Share checkpoints**: Use RunPod Network Storage for team collaboration

---

**Version**: 1.0.0

**Release Date**: November 17, 2025

**Status**: âœ“ Tokenizer training ready, S2S & Emotion coming soon

**License**: MIT - Full commercial use allowed

---

## ğŸ‰ Acknowledgments

This training infrastructure was inspired by:
- **Luna AI** (Pixa) - First Indian speech-to-speech model
- **Sparsh Agrawal** - Proof that world-class AI can be built with limited resources
- **Moshi** (Kyutai Labs) - Real-time duplex architecture
- **GLM-4-Voice** (Tsinghua) - Chinese+English speech model
- **LibriSpeech** - High-quality free speech dataset

Built with â¤ï¸ by developers who believe in **open, independent AI**.

---

**â­ Star the repo to follow Phase 2 (S2S Model) and Phase 3 (Emotions) development!**
