<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Testing-S2S Live</title>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin: 24px; background: #f8f9fa; }
    .container { max-width: 800px; margin: 0 auto; background: white; padding: 24px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
    #status { margin: 12px 0; padding: 8px 12px; border-radius: 4px; font-weight: 500; }
    .status-connected { background: #d4edda; color: #155724; }
    .status-error { background: #f8d7da; color: #721c24; }
    .status-idle { background: #e2e3e5; color: #383d41; }
    .row { margin: 16px 0; }
    label { display: block; margin-bottom: 6px; font-weight: 500; }
    input[type=text] { width: 100%; padding: 8px; border: 1px solid #ced4da; border-radius: 4px; }
    input[type=range] { width: 200px; }
    button { padding: 10px 20px; margin-right: 10px; border: none; border-radius: 4px; cursor: pointer; font-weight: 500; }
    button:disabled { opacity: 0.6; cursor: not-allowed; }
    .btn-start { background: #28a745; color: white; }
    .btn-stop { background: #dc3545; color: white; }
    .btn-mute { background: #6c757d; color: white; }
    #logs { height: 300px; overflow: auto; background: #f8f9fa; border: 1px solid #dee2e6; padding: 12px; font-family: 'Monaco', 'Courier New', monospace; font-size: 12px; white-space: pre-wrap; }
    .audio-controls { margin: 16px 0; padding: 12px; background: #e9ecef; border-radius: 4px; }
    .volume-control { display: inline-block; margin-left: 15px; }
  </style>
</head>
<body>
  <div class="container">
    <h1>üéß Testing-S2S Live</h1>
    <div id="status" class="status-idle">Idle - Click Start to begin</div>
    
    <div class="row">
      <label>WebSocket URL:</label>
      <input id="wsUrl" type="text" value="" placeholder="Auto-detected..." />
    </div>
    
    <div class="audio-controls">
      <div class="row">
        <button id="start" class="btn-start">üé§ Start Audio</button>
        <button id="stop" class="btn-stop" disabled>‚èπÔ∏è Stop</button>
        <button id="mute" class="btn-mute">üîä Mute</button>
        <div class="volume-control">
          <label>Volume:</label>
          <input id="volume" type="range" min="0" max="100" value="90" />
          <span id="volumeValue">90%</span>
        </div>
      </div>
      <small>üìù <strong>Instructions:</strong> Click Start, allow microphone, speak and pause to hear AI response.</small>
    </div>
    
    <div class="row">
      <label>Connection Logs:</label>
      <div id="logs"></div>
    </div>
  </div>

<script>
const statusEl = document.getElementById('status');
const logsEl = document.getElementById('logs');
const wsUrlEl = document.getElementById('wsUrl');
const startBtn = document.getElementById('start');
const stopBtn = document.getElementById('stop');
const muteBtn = document.getElementById('mute');
const volumeSlider = document.getElementById('volume');
const volumeValue = document.getElementById('volumeValue');

let ws = null, media = null, ctx = null, proc = null, gain = null;
let scheduleTime = 0; // Gapless timeline
let isMuted = false;
let audioUnlocked = false; // Track if audio context is unlocked

// Auto-detect WebSocket URL
function detectWebSocketUrl() {
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const host = window.location.host;
    return `${protocol}//${host}/ws/stream`;
}

// Set auto-detected URL on load
window.addEventListener('load', () => {
    const autoUrl = detectWebSocketUrl();
    wsUrlEl.value = autoUrl;
    log(`üîç Auto-detected WebSocket URL: ${autoUrl}`);
});

function setStatus(text, className) {
    statusEl.textContent = text;
    statusEl.className = className;
}

function log(m) { 
    const timestamp = new Date().toLocaleTimeString();
    logsEl.textContent += `[${timestamp}] ${m}\n`; 
    logsEl.scrollTop = logsEl.scrollHeight; 
}

function f32_to_i16(f) {
    const b = new ArrayBuffer(f.length * 2);
    const v = new DataView(b);
    let o = 0;
    for (let i = 0; i < f.length; i++, o += 2) {
        let s = Math.max(-1, Math.min(1, f[i]));
        v.setInt16(o, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
    }
    return b;
}

// Update volume
function updateVolume() {
    const vol = parseInt(volumeSlider.value) / 100;
    volumeValue.textContent = `${volumeSlider.value}%`;
    if (gain && ctx) {
        gain.gain.setValueAtTime(isMuted ? 0 : vol, ctx.currentTime);
        log(`üéöÔ∏è Volume set to ${vol.toFixed(2)} (${isMuted ? 'muted' : 'active'})`);
    }
}

// Toggle mute
function toggleMute() {
    isMuted = !isMuted;
    muteBtn.textContent = isMuted ? 'üîá Unmute' : 'üîä Mute';
    updateVolume();
}

// Play AudioContext unlock tone with multiple attempts
async function unlockAudioContext() {
    if (!ctx) return false;
    if (audioUnlocked) return true;
    
    try {
        // Force resume first
        if (ctx.state === 'suspended') {
            await ctx.resume();
            log('üîÑ AudioContext resumed before unlock');
        }
        
        // Create a tiny 440Hz sine wave (0.15s) at audible but low volume
        const unlockBuffer = ctx.createBuffer(1, ctx.sampleRate * 0.15, ctx.sampleRate);
        const data = unlockBuffer.getChannelData(0);
        for (let i = 0; i < data.length; i++) {
            data[i] = Math.sin(2 * Math.PI * 440 * i / ctx.sampleRate) * 0.1; // Slightly louder
        }
        
        const unlockSource = ctx.createBufferSource();
        unlockSource.buffer = unlockBuffer;
        unlockSource.connect(gain);
        unlockSource.start(ctx.currentTime);
        
        await new Promise((resolve) => {
            unlockSource.onended = () => {
                audioUnlocked = true;
                resolve();
            };
            // Fallback timeout
            setTimeout(resolve, 200);
        });
        
        // Verify audio context is running
        if (ctx.state === 'running') {
            log('üîì ‚úÖ AudioContext unlocked successfully - test tone played');
            audioUnlocked = true;
            return true;
        } else {
            log(`‚ö†Ô∏è AudioContext state: ${ctx.state} after unlock attempt`);
            return false;
        }
    } catch (e) {
        log(`‚ùå Unlock tone failed: ${e.message}`);
        return false;
    }
}

// Play audio frame with gapless scheduling and improved error handling
function playAudioFrame(audioData) {
    if (!ctx || !gain) {
        log('‚ùå Cannot play: AudioContext or gain not initialized');
        return;
    }
    
    // Auto-resume if suspended
    if (ctx.state === 'suspended') {
        ctx.resume().then(() => {
            log('üîÑ Auto-resumed AudioContext for playback');
        });
    }
    
    try {
        // Convert PCM16 to Float32
        const i16 = new Int16Array(audioData);
        const f32 = new Float32Array(i16.length);
        for (let i = 0; i < i16.length; i++) {
            f32[i] = i16[i] / 32768.0;
        }
        
        // Create audio buffer
        const buffer = ctx.createBuffer(1, f32.length, 24000);
        buffer.getChannelData(0).set(f32);
        
        // Gapless scheduling with better timing
        const now = ctx.currentTime;
        if (scheduleTime < now + 0.05) { // Tighter lookahead for better sync
            scheduleTime = now + 0.05; // Small buffer to prevent underruns
        }
        
        // Create source and schedule
        const source = ctx.createBufferSource();
        source.buffer = buffer;
        source.connect(gain);
        source.start(scheduleTime);
        
        // Advance timeline
        const duration = f32.length / 24000;
        scheduleTime += duration;
        
        const durationMs = (duration * 1000).toFixed(1);
        const queueAhead = ((scheduleTime - now) * 1000).toFixed(0);
        log(`üîä Playing: ${i16.length} samples (${durationMs}ms) scheduled at +${queueAhead}ms | ctx=${ctx.state}`);
        
    } catch (e) {
        log(`‚ùå Playback error: ${e.message}`);
        console.error('Audio playback error:', e);
    }
}

async function start() {
    const url = wsUrlEl.value.trim();
    if (!url) { alert('‚ùå Please enter WebSocket URL'); return; }
    
    try {
        log('üîå Connecting to WebSocket...');
        setStatus('Initializing...', 'status-idle');
        
        // Setup AudioContext
        ctx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });
        
        // Force resume and wait for it
        if (ctx.state === 'suspended') {
            log('üîÑ Resuming suspended AudioContext...');
            await ctx.resume();
        }
        
        // Verify AudioContext is actually running
        if (ctx.state !== 'running') {
            throw new Error(`AudioContext state is ${ctx.state}, expected 'running'`);
        }
        
        log(`üéµ AudioContext: ${ctx.state}, ${ctx.sampleRate} Hz`);
        
        // Create gain node
        gain = ctx.createGain();
        gain.connect(ctx.destination);
        updateVolume();
        log('üéöÔ∏è Gain node connected to destination');
        
        // Initialize gapless timeline
        scheduleTime = ctx.currentTime;
        log(`‚è∞ Timeline initialized at ${scheduleTime.toFixed(3)}s`);
        
        // Play unlock tone to ensure audio path is active (CRITICAL for browser audio)
        const unlocked = await unlockAudioContext();
        if (!unlocked) {
            log('‚ö†Ô∏è Audio unlock may have failed - you might need to interact with the page');
        }
        
        // Double-check AudioContext is running
        if (ctx.state !== 'running') {
            await ctx.resume();
            log(`üîÑ Force resumed AudioContext: ${ctx.state}`);
        }
        
        // Connect WebSocket
        setStatus('Connecting to server...', 'status-idle');
        ws = new WebSocket(url);
        ws.binaryType = 'arraybuffer';
        
        ws.onopen = () => {
            log('‚úÖ WebSocket connected successfully');
            setStatus('Connected - Ready for audio', 'status-connected');
        };
        
        ws.onclose = (event) => {
            log(`‚ùå WebSocket closed (code: ${event.code}, reason: ${event.reason || 'unknown'})`);
            setStatus('Connection closed', 'status-error');
        };
        
        ws.onerror = (error) => {
            log('‚ùå WebSocket error occurred');
            console.error('WebSocket error:', error);
            setStatus('Connection error', 'status-error');
        };
        
        ws.onmessage = (ev) => {
            if (!(ev.data instanceof ArrayBuffer)) {
                log('‚ö†Ô∏è Received non-binary data');
                return;
            }
            
            // Ensure AudioContext is running before playback
            if (ctx.state === 'suspended') {
                ctx.resume().then(() => {
                    log('üîÑ Auto-resumed for incoming audio');
                    playAudioFrame(ev.data);
                });
            } else {
                // Play received audio frame
                playAudioFrame(ev.data);
            }
        };
        
        // Setup microphone
        log('üé§ Requesting microphone access...');
        media = await navigator.mediaDevices.getUserMedia({
            audio: {
                channelCount: 1,
                sampleRate: 24000,
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            }
        });
        
        log('‚úÖ Microphone access granted');
        
        // Setup input processing
        const source = ctx.createMediaStreamSource(media);
        proc = ctx.createScriptProcessor(2048, 1, 1);
        
        proc.onaudioprocess = (e) => {
            if (!ws || ws.readyState !== WebSocket.OPEN) return;
            
            // Monitor AudioContext state and resume if needed
            if (ctx.state === 'suspended') {
                ctx.resume();
                log('üîÑ Resumed AudioContext during recording');
            }
            
            const inputData = e.inputBuffer.getChannelData(0);
            const chunkSize = 1920; // 80ms at 24kHz
            
            for (let i = 0; i < inputData.length; i += chunkSize) {
                const slice = inputData.subarray(i, Math.min(i + chunkSize, inputData.length));
                if (slice.length > 0) {
                    const buffer = f32_to_i16(slice);
                    ws.send(buffer);
                }
            }
        };
        
        // CRITICAL FIX: Do NOT connect input processor to destination!
        // This was causing feedback and blocking playback
        source.connect(proc);
        // proc.connect(ctx.destination); // REMOVED - input should not play to speakers
        
        log('üéµ Audio pipeline ready - speak now!');
        setStatus('üé§ Listening... (speak and pause for response)', 'status-connected');
        
        startBtn.disabled = true;
        stopBtn.disabled = false;
        
    } catch (err) {
        log(`‚ùå Setup error: ${err.message}`);
        setStatus('Setup failed', 'status-error');
        console.error('Setup error:', err);
        stop();
    }
}

function stop() {
    if (proc) {
        proc.disconnect();
        proc = null;
        log('üîá Audio processor disconnected');
    }
    if (gain) {
        gain.disconnect();
        gain = null;
        log('üéöÔ∏è Gain node disconnected');
    }
    if (ctx && ctx.state !== 'closed') {
        ctx.close();
        ctx = null;
        log('üîá Audio context closed');
    }
    if (media) {
        media.getTracks().forEach(track => {
            track.stop();
            log(`üõë Stopped ${track.kind} track`);
        });
        media = null;
    }
    if (ws) {
        ws.close();
        ws = null;
        log('üîå WebSocket disconnected');
    }
    
    scheduleTime = 0;
    audioUnlocked = false; // Reset for next session
    startBtn.disabled = false;
    stopBtn.disabled = true;
    setStatus('Stopped', 'status-idle');
}

startBtn.onclick = start;
stopBtn.onclick = stop;
muteBtn.onclick = toggleMute;
volumeSlider.oninput = updateVolume;

// Auto-resume on tab focus
document.addEventListener('visibilitychange', () => {
    if (!document.hidden && ctx && ctx.state === 'suspended') {
        ctx.resume().then(() => {
            log('üîÑ AudioContext resumed after tab focus');
        });
    }
});

// Initialize
document.addEventListener('DOMContentLoaded', () => {
    log('üöÄ Testing-S2S Web Interface Ready');
    log('üí° Tip: Use REPLY_MODE=turn for conversation mode');
});
</script>
</body>
</html>